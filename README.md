
# ğŸ§  Custom GPT-2 Pretraining & Text Generation

This project fine-tunes **GPT-2** on custom domain datasets such as *Cricket*, *Education*, *Medical*, and more.
By training GPT-2 on your own `.txt` files, the model becomes better at generating domain-aware, context-rich text.

The repository includes:

* âœ”ï¸ Pretraining script (`Pretraining_gpt2.py`)
* âœ”ï¸ Inference script (`hf_inference.py`)
* âœ”ï¸ Custom datasets (`*.txt` files)
* âœ”ï¸ Example usage
* âœ”ï¸ Instructions to reproduce training & inference



## ğŸ“Œ What This Model Does

This project performs **domain-adapted pretraining** (continued training) on GPT-2 using Hugging Faceâ€™s Transformers library.

After training, the model can:

* Generate text aligned with your datasets
* Answer questions about cricket, education, medical topics, etc.
* Produce GPT-2-style completions based on your training data
* Learn vocabulary and patterns found in your custom files

All trained model files are stored in:


trained_model/




## ğŸ“‚ Project Structure



â”œâ”€â”€ Cricket.txt
â”œâ”€â”€ Education.txt
â”œâ”€â”€ Medical.txt
â”œâ”€â”€ Pretraining_gpt2.py
â”œâ”€â”€ hf_inference.py
â”œâ”€â”€ test_gpt2.py
â””â”€â”€ trained_model/  (generated after training)




## âš™ï¸ Installation

First, install required dependencies:

bash
pip install transformers datasets accelerate safetensors


(Optional for GPU acceleration)

bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121




## ğŸš€ Training the Model

Run the pretraining script:

bash
python Pretraining_gpt2.py


This script:

* Loads all `.txt` files in the current directory
* Tokenizes your data
* Trains GPT-2 using Causal Language Modeling
* Saves the final model to `trained_model/`

After training finishes, you will see:


trained_model/
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ config.json
    â”œâ”€â”€ tokenizer.json
    â”œâ”€â”€ vocab.json
    â””â”€â”€ merges.txt




## ğŸ¤– Running Inference

Use `hf_inference.py` to generate text from your trained model.

### Run:

bash
python hf_inference.py


Make sure the script points to your trained model:

python
model_path = "trained_model"


You can modify the prompt inside the script to test different inputs.



## ğŸ“ Example Inputs & Outputs

### Input:


Explain the basics of cricket batting:


### Output:


Cricket batting involves proper stance, balance, and footwork.
A batsman should watch the ball closely, judge the length early,
and choose between a defensive or attacking shot. Timing and practice
help improve consistency and shot selection.




### Input:


What is Artificial Intelligence?


### Output:


Artificial Intelligence refers to the ability of computers and machines
to perform tasks that typically require human intelligence, such as learning,
reasoning, decision making, and natural language understanding.




## ğŸ§ª Testing the Model

You may use `test_gpt2.py` (or inference script) to try out various prompts and test generations.



## ğŸ“š Technologies Used

* **Python**
* **Hugging Face Transformers**
* **Datasets Library**
* **PyTorch**
* **GPT-2**
